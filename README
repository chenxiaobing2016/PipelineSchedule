## Methods To Be Deployed:
### Split workload into several pieces.
### fine-grained schedule in each stage.
### coarse-grained schedule between stages and pipelines.
### peephole optimizations to reduce io (or fuse several instrutions into one).


## Scenario:
For GPU Processors, tasks could be devided into several subtasks. Commonly, Each subtask (pipeline) is made up with three stages, e.g. Load, compute and store.
Operations among stages(StageWorkLoad) should be executed in order, While Operations in each Stage could be executed orderly or disorderly. For instance, In loading stage, two block of data need to be loaded. For data A, it should be firstly loaded from global memory into shared memory(op1), and then loaded from shared memory into local memory(op2). For data B, it should be loaded from global memory into local memory directly.

Data A: [ global memory ] --(op1)--> [ shared memory ]  --(op2)--> [ local memory ]
Data B: [ global memory ] ----------------(op3)------------------> [ local memory ]

Suppose opx could be conducted by DMAx, op1 or op2 could be loaded parallelly with op3. Since there exists data conflict between op1 and op2, op2 must be conducted after op1.
