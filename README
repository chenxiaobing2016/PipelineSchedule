Methods To Be Deployed:
* Split workload into several pieces.
* fine-grained schedule in each stage.
* coarse-grained schedule between stages and pipelines.
* peephole optimizations to reduce io (or fuse several instrutions into one).


Scenario:
For GPU Processors, tasks could be devided into several subtasks. Commonly, Each subtask (pipeline) is made up with three stages, e.g. Load, compute and store.
Operations among stages(StageWorkLoad) should be executed in order, While Operations in each Stage could be executed orderly or disorderly. For instance, In loading stage, two block of data need to be loaded. For data A, it should be firstly loaded from global memory into shared memory(op1), and then loaded from shared memory into local memory(op2). For data B, it should be loaded from global memory into local memory directly.

Data A: [ global memory ] --(op1)--> [ shared memory ]  --(op2)--> [ local memory ]
Data B: [ global memory ] ----------------(op3)------------------> [ local memory ]

Suppose opx could be conducted by DMAx, op1 or op2 could be loaded parallelly with op3. Since there exists data conflict between op1 and op2, op2 must be conducted after op1.


Comparsion Metrics could be used:
* Running time of the Algorithms
* Number of Occurrences of Better Quality of Schedules.
* Speedup: the ratio of sequential executation times to makespan.
* Schedule Length Ratio(SLR): the ratio of makespan to the summation of computation costs for tasks in critical path.
* efficiency: speedup and makespan ratio.


Model Structure:
In our model, there are several types of function units, in which each type only support one kind of operation. A processor may include more than one function unit for each type. All function unit are connectted with configurable bandwidth. But the bandwidths between same kind of fu are the same. The number of function units and the type of each function unit can be configed by hardware designers.
And a task(neural-network) is abstracted as several operations with data dependence among operations, and one specific layer could be reckoned by one kind of fu.

Random Graph Generator Parameters:
* Number of original task graph (v)
* Shape paramter of the graph (alpha).
  The height of graph is generated randomly from a uniform distribution with a mean value equal to (sqrt(v) / alpha), and the width of each level is randomly generated from another uniform distribution with a mean value (sqrt(v) * alpha).
* out degree of each node.
* Communication to Computation ratio(CCR).
* Heterogeneity of processors (beta).
  Suppose a_w is the average computation cost,  the average computation cost of a function unit is randomly generated from a uniform distribution from [0, 2 * avg], the computation cost of each task ti on processor pj is randomly set from the range [wj*(1-beta/2), wj*(1+beta/2)]


New Random Graph Generator Paramters:
* height of the graph (height)
* width of the graph (width)
* the initial input size (input_size)
* the out degree of concat and slice (out_degree)
* the shrink rate of conv, fc, pool, and other layers shrink rate is 1 (out_rate)

Processor Generater:
* for a given graph, the average of compute rate of one kind of processor is positive relate with average size of responded layers in the graph, the positive ratio (P);
* Communication to Computation ratio of specific kinds of processor(CCR).
